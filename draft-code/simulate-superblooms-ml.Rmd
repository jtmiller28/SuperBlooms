---
title: "simulate-superblooms-ml"
author: "jt-miller"
date: "2025-06-04"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Simulate Superblooms and use classic inference with Maximum Likelihood 
```{r}
library(tidyverse)
library(pROC)
```

### Try and create the most basic hierarchical model of flowering abundance 
Assumptions <br>
- Each year $y$ (for y = 1, ...,30) flowering follows a unimodal phenology curve <br>
- The number of flowering individuals on day-of-year (DOY) $d$ is normally distributed around a year specific phenological mean  $\mu_y$, with year-specific standard deviation $\sigma_y$ which can represent the duration of flowering(?). <br>
- The total flowering abundance per year may vary, modeled as a latent year-specific abundance term $N_y$ <br>

Let, <br>
- $A_{y,d}$ be the abundance of flowering indivduals on day $d$ of year $y$ 
- $N_y$ be the total flowering abudance in year $y$
- $\mu_y$ be the mean flowering date for year $y$ 
- $\sigma_y$ be the duration of flowering period for year $y$ (?)

Math to form a Gaussian phenology curve per year:
$$
A_{y,d} = N_y \times NormalPDF(d|\mu_y,\sigma_y) \\ 
where, \\ 
NormalPDF(d|\mu,\sigma) = \frac{1}{\sqrt2{\pi\sigma}} exp(-\frac{1}{2\sigma^2}(d - \mu)^2)
$$
Create a simu of this basic model, note that I'll be using hyperparameters, because peak flowering day (mu_y) and flowering duration (sigma_y) vary across years but are not completely independent. They are samples from higher level (hyper) distributions. These hyperparameters describe the distributions *from which year-specific parameters are drawn*. 

E.g. If most years have peak flowering around doy 100, then we model: 
$$
\mu_y \sim Normal(\mu_\mu, \sigma_\mu)
$$
where $\mu_\mu$ is the long term average phenological mean, and $\sigma_\mu$ reflects interannual variability. These define the prior distributions for year-level parameters. 
```{r}
library(ggplot2)
library(dplyr)
 
# define simu parms
n_years <- 30
days <- 1:365

# set up hyperparms 
mu_mu <- 100 # average flowering peak across years
sigma_mu <- 10 # variability in peak flowering phenology across years
mu_sigma <- 15 # spread/duration of flowering
sigma_sigma <- 5 # spread/duration of flowering 
mu_logN <- 9 # log total abundance 
sigma_logN <- 0.5 # log sigma 

# Simu yearly parms 
mu_y <- rnorm(n_years, mean = mu_mu, sd = sigma_mu) # draw each year's flowering mean mu_y for Normal Distributions defined by the hyperparameters mu_mu & sigma_mu
sigma_y <- abs(rnorm(n_years, mean = mu_sigma, sd = sigma_sigma)) # draw each year's standard deviation sigma_y from Normal distributions defined by the hyperparameters mu_sigma and sigma_sigma
N_y <- rlnorm(n_years, meanlog = mu_logN, sdlog = sigma_logN) # simulate N_y using a log-normal distribution


# Simulate flowering abundance curves per year
sim.data.gen <- do.call(rbind, lapply(1:n_years, function(y) {
  doy_density <- dnorm(days, mean = mu_y[y], sd = sigma_y[y])
  A_yd <- N_y[y] * doy_density
  
  data.frame(
    year = y, 
    doy = days, 
    abundance = A_yd
  )
}))

# Visualize a subset of years
sim.data.gen %>% 
  filter(year %in% c(1,5,10,20, 30)) %>% 
  ggplot(aes(x = doy, y = abundance, color = factor(year))) +
  geom_line() +
  labs(title = "Simu Flowering Abudundance Curves", 
       x = "Day of Year", y = "Abundance", color = "Year") + 
    theme_minimal()
```



### Additionally, we must consider an observation process for these data. Lets go with the simplest, being that we are obtaining count data from a Poisson distribution pull.
$$
Counts_{y,d} \sim Poisson(\lambda_{y,d}) \\
\lambda_{y,d} = A_{y,d} = N_y \times NormalPDF(d|\mu_y,\sigma_y)
$$
```{r}
# define temporal parms
n_years <- 30
days <- 1:365

# set up simu parms
mu_mu <- 100 # average flowering peak across years
sigma_mu <- 20 # variability in peak flowering phenology across years
mu_sigma <- 15 # spread/duration of flowering
sigma_sigma <- 5 # spread/duration of flowering 
mu_logN <- 9 # log total abundance 
sigma_logN <- 0.5 # log sigma 

# Simu yearly parms 
mu_y <- rnorm(n_years, mean = mu_mu, sd = sigma_mu) # draw each year's flowering mean mu_y for Normal Distributions defined by the hyperparameters mu_mu & sigma_mu
sigma_y <- abs(rnorm(n_years, mean = mu_sigma, sd = sigma_sigma)) # draw each year's standard deviation sigma_y from Normal distributions defined by the hyperparameters mu_sigma and sigma_sigma
N_y <- rlnorm(n_years, meanlog = mu_logN, sdlog = sigma_logN) # simulate N_y using a log-normal distribution


# Simulate count observations 
sim.data.gen.obs <- purrr::map_dfr(1:n_years, function(y){
  doy_density <- dnorm(days, mean = mu_y[y], sd = sigma_y[y]) # normal pdf
  A_yd <- N_y[y] * doy_density # expected abundance 
  count_obs <- rpois(length(A_yd), lambda = A_yd) # Poisson observation step
  tibble(
    year = y, 
    doy = days, 
    abundance = A_yd, # true expected abd
    count_obs = count_obs # observed counts
  )
})

# Aggregate over 5 day time bins 
sim.data.gen.obs.binned <- sim.data.gen.obs %>%
  mutate(doy_bin = cut(doy, breaks = seq(0, 365, by = 5))) %>%
  group_by(year, doy_bin) %>%
  summarise(
    doy_mid = mean(doy),
    abundance = sum(abundance),
    count_obs = sum(count_obs),
    .groups = "drop"
  )

bin_width <- 5 # create a binwidth that matches our binned data interval
sim.data.gen.obs.binned %>%
  filter(year %in% c(5, 20, 30)) %>%
  ggplot() +
  geom_col(aes(x = doy_mid, y = count_obs, fill = factor(year)), 
           position = "identity", width = bin_width * 0.9, alpha = 0.4) +
  geom_line(aes(x = doy_mid, y = abundance, color = factor(year)), linewidth = 1) +
  labs(title = "Flowering Abundance vs Observed Counts (Binned by 5 day increments)",
       x = "Day of Year", y = "Count / Expected Abundance", color = "Year", fill = "Year") +
  theme_minimal()
```
Currently N_y is constructed is such a way that it just randomly draws from a log-normal distribution. This means that we are 'randomly' experiencing high abundance flowering years vs low abundance flowering years. <br>

### We can expand this model (without yet adding biology) by additional latent superbloom indicator parameter S_y. 
$$
S_y \sim Bernoulli(p) \\
log(N_y) = \alpha_0 + \alpha_1 \times S_y \\
\lambda_{y,d} = A_{y,d} = N_y \times NormalPDF(d|\mu_y,\sigma_y) \\
Counts_{y,d} \sim Poisson(\lambda_{y,d}) \\
$$
Here for our data generative model, we're assuming that there are two different regimes of flowering. Normal years where S_y = 0, such that log(N_y) = alpha_0 resulting in moderate/low abundance of flowering. Superbloom years where S_y = 1, such that log(N_y) = alpha_0 + alpha_1 resulting in high abundance of flowering. This is a two component mixture model on N_y where S_y chooses which distribution to draw from and alpha_1 defines how extreme superblooms are. <br>

For the observation model, we're going to retain our random draws from a Poisson distribution to keep things simple for now. 
```{r}
# define temporal parms
n_years <- 30
days <- 1:365

# set up simu parms
mu_mu <- 100 # average flowering peak across years
sigma_mu <- 20 # variability in peak flowering phenology across years
mu_sigma <- 15 # spread/duration of flowering
sigma_sigma <- 5 # spread/duration of flowering 
alpha_0 <- 9 # means that N_y ~ exp(9) in normal years
alpha_1 <- 2.5 # means that N_y ~ exp(9 + 2.5) in superbloom years
p_bloom <- 0.20 # the fixed prob of blooming 

# Simu yearly parms 
mu_y <- rnorm(n_years, mean = mu_mu, sd = sigma_mu) # draw each year's flowering mean mu_y for Normal Distributions defined by the hyperparameters mu_mu & sigma_mu
sigma_y <- abs(rnorm(n_years, mean = mu_sigma, sd = sigma_sigma)) # draw each year's standard deviation sigma_y from Normal distributions defined by the hyperparameters mu_sigma and sigma_sigma
S_y <- rbinom(n_years, size = 1, prob = p_bloom) # latent superbloom indicator variable
log_N_y <- alpha_0 + alpha_1 * S_y # calc the abd (to be exponentiated) based on whether superbloom occurs or not. 
N_y <- exp(log_N_y)

# simulate count data with latent superbloom indicator S_y
sim.latent <- purrr::map_dfr(1:n_years, function(y){
  doy_density <- dnorm(days, mean = mu_y[y], sd = sigma_y[y]) # pdf over doy 
  A_yd <- N_y[y] * doy_density # expected abd lambda_{y,d}
  count_obs <- rpois(length(A_yd), lambda = A_yd) # Poisson count draw
  # construct summary
  tibble( 
    year = y, 
    doy = days, 
    mu_y = mu_y[y],
    sigma_y = sigma_y[y], 
    S_y = S_y[y], 
    N_y = N_y[y], 
    abundance = A_yd, 
    count_obs = count_obs
  )
}
  )

# Aggregate over 5 day time bins 
sim.latent.binned <- sim.latent %>%
  mutate(doy_bin = cut(doy, breaks = seq(0, 365, by = 5))) %>%
  group_by(year, doy_bin) %>%
  summarise(
    doy_mid = mean(doy),
    abundance = sum(abundance),
    count_obs = sum(count_obs),
    S_y = unique(S_y),
    .groups = "drop"
  )

bin_width <- 5 # create a binwidth that matches our binned data interval
sim.latent.binned %>%
  filter(year %in% c(5, 20, 30)) %>%
  ggplot() +
  geom_col(aes(x = doy_mid, y = count_obs, fill = factor(year)), 
           position = "identity", width = bin_width * 0.9, alpha = 0.4) +
  geom_line(aes(x = doy_mid, y = abundance, color = factor(year), linetype = factor(S_y)), linewidth = 1) +
  scale_linetype_manual(values = c("solid", "longdash"), 
                        labels = c("Normal", "Superbloom"), 
                        name = "Bloom type") +
  labs(title = "Flowering Abundance vs Observed Counts \n With Superbloom Latent Indicator Parameter is Added \n (Binned by 5 day increments)",
       x = "Day of Year", y = "Count / Expected Abundance", color = "Year", fill = "Year") +
  theme_minimal()

```
### Fit linear models and make inference 
Here, I want to test some methods in fitting linear models and trying to use methods from maximum likelihood to derive information. The goal will be to be able to identify what years are superblooms, what the average flowering peak is among years (mu_y), and the variability in peak flowering among years (sigma_y) <br>

Now lets start simple, using basic linear models lets fit a polynomial
```{r}
# test a particular year to see if we can fit the model correctly: 
test.sim.latent <- filter(sim.latent, year == 20)

# fit a polynomial lm to these data (note that raw can be set to FALSE, same solutions but some stuff online about whetehr this is numericall)
test_poly_fit <- lm(count_obs ~ poly(doy, degree = 2), data = test.sim.latent)

# Predict counts using the fitted model
test.sim.latent <- test.sim.latent %>%
  mutate(predicted_count = predict(test_poly_fit, type = "response"))
# try plotting to see how we did 
ggplot(test.sim.latent) + 
  geom_col(mapping = aes(x = doy, y = count_obs), 
           alpha = 0.4) + 
  geom_line(mapping = aes(x= doy, y = predicted_count), color = "goldenrod")
  
```
Well thats just terrible, after some reading it appears that the problem is that lms cant deal with zero inflation, which our count data has an abundance of. Lets try seeing if a glm can provide a better fit for these data. 
```{r}
# fit a polynomial glm to these data
glm_test_poly_fit<- glm(count_obs ~ poly(doy, degree = 2), 
                        family = "poisson",  data = test.sim.latent)

# Predict counts using the fitted model
test.sim.latent <- test.sim.latent %>%
  mutate(predicted_count = predict(glm_test_poly_fit, type = "response")) # important to use response here, as it scales our expected counts rather than the log scale counts for the predicted values

ggplot(test.sim.latent) + 
  geom_col(mapping = aes(x = doy, y = count_obs), 
           alpha = 0.4) + 
  geom_line(mapping = aes(x= doy, y = predicted_count), color = "goldenrod")

```
### Assess Model Fit (Bootstrap)

### Extract mu_y & mu_sigma
unfortunately with a polynomial, we cannot extract these directly as z_1 and z_2 are orthogonal polynomials, not parameters of the normal pdf
$$
log(\lambda_d) = \beta_0 + \beta_1 z_1(doy) + \beta_2z_2(doy)
$$
So we can estimate mu_y and sigma_y from the predictions
```{r}
# mu_y est is just the maximum value (peak flowering)
mu_y_hat <- test.sim.latent$doy[which.max(test.sim.latent$predicted_count)] 

# sigma_y is the standard deviation around the peak flowering
pred <- test.sim.latent$predicted_count
pred_norm <- pred / sum(pred) # normalize predicted counts
sigma_y_hat <- sqrt(sum((test.sim.latent$doy - mu_y_hat)^2 * pred_norm))

# How far is this from the truth of the simulated model run?
mu_y_hat_known <- unique(test.sim.latent$mu_y)
print(paste(round(((mu_y_hat - mu_y_hat_known)/mu_y_hat_known) * 100, 3), "% error for mu_y estimation"))
sigma_y_hat_known <- unique(test.sim.latent$sigma_y)
print(paste(round(((sigma_y_hat - sigma_y_hat_known)/sigma_y_hat_known) * 100, 3), "% error for sigma_y estimation"))
```

Can we extract the MLEs?
```{r}
mles <- coef(glm_test_poly_fit)
print(mles)
```
No...due to the same orthogonal polynomial coefficient issue. Oh well...

Now address whether we can pull out whether S_y = 1 or 0, indicating whether there is a superbloom occurring. Lets tr
```{r}
# first, summarize each year in our simulation
yearly_summary <- sim.latent %>%
  group_by(year) %>%
  summarise(
    total_counts = sum(count_obs),
    peak_doy = doy[which.max(count_obs)],
    count_sd = sd(count_obs),
    S_y = unique(S_y),
    .groups = "drop"
  )

glm_superbloom <- glm(S_y ~ total_counts + peak_doy + count_sd,
                      family = "binomial",
                      data = yearly_summary)

summary(glm_superbloom)

# predict superblooms via classification on our glm
yearly_summary <- yearly_summary %>%
  mutate(
    S_y_hat_prob = predict(glm_superbloom, type = "response"),
    S_y_hat = ifelse(S_y_hat_prob > 0.5, 1, 0)
  )

# confusion matrix 
confusion_matrix <- table(True = yearly_summary$S_y, Predicted = yearly_summary$S_y_hat)
print(confusion_matrix)
print(paste0((mean(yearly_summary$S_y == yearly_summary$S_y_hat)*100), "% Accuracy in Predicting SuperBlooms")) # overall accuracy
```
So under ideal conditions, we can use the total counts, peak flowering in day of year, and the deviation of flowering to infer when superblooms occur according to our model with 100% accuracy. 

### Package this into a function, see how messing with params changes our prediction of superblooms
```{r}
sim.latent.fxn <- function(n_years = 30, # number of years to simu
                         days = 1:365, # doy 
                         mu_peak = 100, # center for mean flowering
                         dur = 12, # duration of flowering 
                         y_var = 0, # variability per year
                         alpha_0 = 9, # standard bloom
                         alpha_1 = 2.5, # additional bloom param
                         p1 = 2, # these two make the expected value 20% 
                         p2 = 8 
                         ){
# define temporal parms
n_years <- n_years
days <- days

# set up simu parms (shifting to hyperparameters)
mu_mu <- rnorm(1, mean = mu_peak, sd = 10) # mean DOY peak across years
sigma_mu <- abs(rnorm(1, mean = y_var, sd = 5)) # variability across years
mu_sigma <- rnorm(1, mean = dur, sd = 3) # spread/duration of flowering 
sigma_sigma <- abs(rnorm(1, mean = 0, sd = 3)) # spread/duration of flowering
alpha_0 <- alpha_0 # means that N_y ~ exp(9) in normal years
alpha_1 <- alpha_1 # means that N_y ~ exp(9 + 2.5) in superbloom years

p_bloom <- rbeta(1, shape1 = p1, shape2 = p2) # beta distribution, the conjugate prior for the Bernoulli and Binomial, 
# year specific parameters
mu_y <- rnorm(n_years, mean = mu_mu, sd = sigma_mu) # draw each year's flowering mean mu_y for Normal Distributions defined by the hyperparameters mu_mu & sigma_mu
sigma_y <- abs(rnorm(n_years, mean = mu_sigma, sd = sigma_sigma)) # draw each year's standard deviation sigma_y from Normal distributions defined by the hyperparameters mu_sigma and sigma_sigma
S_y <- rbinom(n_years, size = 1, prob = p_bloom) # draw from a binomial distribution for whether this should be a superbloom year, given the probability of the superbloom. 

log_N_y <- alpha_0 + alpha_1 * S_y # Determine log-abundance of flowering
N_y <- exp(log_N_y) # Exponentiate

# simulate count data with latent superbloom indicator S_y
sim.latent <- purrr::map_dfr(1:n_years, function(y){
  doy_density <- dnorm(days, mean = mu_y[y], sd = sigma_y[y]) # pdf over doy 
  A_yd <- N_y[y] * doy_density # expected abd lambda_{y,d}
  count_obs <- rpois(length(A_yd), lambda = A_yd) # Poisson count draw
  # construct summary
  tibble( 
    year = y, 
    doy = days, 
    mu_y = mu_y[y],
    sigma_y = sigma_y[y], 
    S_y = S_y[y], 
    N_y = N_y[y], 
    abundance = A_yd, 
    count_obs = count_obs
  )
}
  )
return(sim.latent)
}

check.superbloom.assessment <- function(data = data){
  # first, summarize each year in our simulation
yearly_summary <- data %>%
  group_by(year) %>%
  summarise(
    total_counts = sum(count_obs),
    peak_doy = doy[which.max(count_obs)],
    count_sd = sd(count_obs),
    S_y = unique(S_y),
    .groups = "drop"
  )

glm_superbloom <- glm(S_y ~ total_counts + peak_doy + count_sd,
                      family = "binomial",
                      data = yearly_summary)

# predict superblooms via classification on our glm
yearly_summary <- yearly_summary %>%
  mutate(
    S_y_hat_prob = predict(glm_superbloom, type = "response"),
    S_y_hat = ifelse(S_y_hat_prob > 0.5, 1, 0)
  )

# confusion matrix 
confusion_matrix <- table(True = yearly_summary$S_y, Predicted = yearly_summary$S_y_hat)
print(confusion_matrix)
print(paste0((mean(yearly_summary$S_y == yearly_summary$S_y_hat)*100), "% Accuracy in Predicting SuperBlooms")) # overall accuracy
return(mean(yearly_summary$S_y == yearly_summary$S_y_hat)*100)
}
```
Check to see what params tweaked change our detection rate of superblooms. 
```{r eval=FALSE, include=TRUE}
# try simulating across varying the vars in order to see how these effect our predictive process for superblooms: 

param_grid <- expand.grid(n_years = seq(2, 30, by = 2), 
                          mu_peak = seq(55, 125, by = 10),
                          y_var = seq(0, 50, by = 5))
results_df <- tibble()
for(i in 1:nrow(param_grid)){
  params <- param_grid[i, ]
sim1 <- sim.latent.fxn(n_years = params$n_years, # number of years to simu
                         days = 1:150, # doy 
                         mu_peak = params$mu_peak, # center for mean flowering
                         dur = 12, # duration of flowering 
                         y_var = params$y_var, # variability per year
                         alpha_0 = 9, # standard bloom
                         alpha_1 = 2.5, # additional bloom param
                         p1 = 2, # these two make the expected value 20% 
                         p2 = 8 
                       )  

# try plot 
# sim1 %>%
#   filter(year %in% c(5, 10, 15, 20, 30)) %>%
#   ggplot() +
#   geom_col(aes(x = doy, y = count_obs, fill = factor(year)), 
#            position = "identity", width = bin_width * 0.9, alpha = 0.1) +
#   geom_line(aes(x = doy, y = abundance, color = factor(year), linetype = factor(S_y)), linewidth = 1) +
#   scale_linetype_manual(values = c("solid", "longdash"), 
#                         labels = c("Normal", "Superbloom"), 
#                         name = "Bloom type") +
#   labs(title = "Flowering Abundance vs Observed Counts \n With Superbloom Latent Indicator Parameter is Added",
#        x = "Day of Year", y = "Count / Expected Abundance", color = "Year", fill = "Year") +
#   theme_minimal()

accuracy <- check.superbloom.assessment(data = sim1)
results_df <- bind_rows(results_df, 
                        tibble(
                          n_years = params$n_years, 
                          mu_peak = params$mu_peak, 
                          y_var = params$y_var,
                          accuracy = accuracy
                        ))
}
```
Now lets start messing with our observation process. Currently we assume a pull from the Poisson distribution. Lets change this by introducing a detection rate, thinning the Poisson count pull.
$$
S_y \sim Bernoulli(p) \\
log(N_y) = \alpha_0 + \alpha_1 \times S_y \\
\lambda_{y,d} = A_{y,d} = N_y \times NormalPDF(d|\mu_y,\sigma_y) \\
TrueCounts_{y,d} \sim Poisson(\lambda_{y,d}) \\
ObservedCounts_{y,d} \sim Binomial(TrueCounts_{y,d}, pdet))
$$
Here, the detection probability (pdet) will be a constant we can vary. 
```{r}
sim.latent.w.det.fxn <- function(n_years = 30, # number of years to simu
                         days = 1:365, # doy 
                         mu_peak = 100, # center for mean flowering
                         dur = 12, # duration of flowering 
                         y_var = 0, # variability per year
                         alpha_0 = 9, # standard bloom
                         alpha_1 = 2.5, # additional bloom param
                         pbloom = 0.2, # the prob of superbloom
                         pdet = 0.5 # the observation rate 
                         ){
# define temporal parms
n_years <- n_years
days <- days

# set up simu parms (shifting to hyperparameters)
mu_mu <- rnorm(1, mean = mu_peak, sd = 10) # mean DOY peak across years
sigma_mu <- abs(rnorm(1, mean = y_var, sd = 5)) # variability across years
mu_sigma <- rnorm(1, mean = dur, sd = 3) # spread/duration of flowering 
sigma_sigma <- abs(rnorm(1, mean = 0, sd = 3)) # spread/duration of flowering
alpha_0 <- alpha_0 # means that N_y ~ exp(9) in normal years
alpha_1 <- alpha_1 # means that N_y ~ exp(9 + 2.5) in superbloom years

calibrate_bloom_beta <- function(mean, fixed_shape1 = 2) {
  if (mean <= 0 || mean >= 1) stop("mean must be strictly between 0 and 1")
  shape1 <- fixed_shape1
  shape2 <- shape1 * (1 - mean) / mean
  return(list(pshape1 = shape1, pshape2 = shape2))
}
bloom_params <- calibrate_bloom_beta(mean = pbloom, fixed_shape1 = 2)
p_bloom <- rbeta(1, shape1 = bloom_params$pshape1, shape2 = bloom_params$pshape2) # beta distribution, the conjugate prior for the Bernoulli and Binomial, 
# year specific parameters
mu_y <- rnorm(n_years, mean = mu_mu, sd = sigma_mu) # draw each year's flowering mean mu_y for Normal Distributions defined by the hyperparameters mu_mu & sigma_mu
sigma_y <- abs(rnorm(n_years, mean = mu_sigma, sd = sigma_sigma)) # draw each year's standard deviation sigma_y from Normal distributions defined by the hyperparameters mu_sigma and sigma_sigma
S_y <- rbinom(n_years, size = 1, prob = p_bloom) # draw from a binomial distribution for whether this should be a superbloom year, given the probability of the superbloom. 

log_N_y <- alpha_0 + alpha_1 * S_y # Determine log-abundance of flowering
N_y <- exp(log_N_y) # Exponentiate

# simulate count data with latent superbloom indicator S_y
sim.latent.det <- purrr::map_dfr(1:n_years, function(y){
  doy_density <- dnorm(days, mean = mu_y[y], sd = sigma_y[y]) # pdf over doy 
  A_yd <- N_y[y] * doy_density # expected abd lambda_{y,d}
  true_count <- rpois(length(A_yd), lambda = A_yd) # Poisson count draw
  # make an observation process rate
  # Helper function to calibrate beta distribution for desired mean
  
calibrate_beta <- function(mean, fixed_shape1 = 10) { # build a internal fxn that allows for setting the detection rate as a percentage.
  shape1 <- fixed_shape1
  shape2 <- shape1 * (1 - mean) / mean
  return(list(shape1 = shape1, shape2 = shape2))
}
beta_params <- calibrate_beta(pdet)  # Allows for input of decimal percentage
pdet_y <- rbeta(n_years, shape1 = beta_params$shape1, shape2 = beta_params$shape2) # Vary across years, but have an expected value around pdet input
obs_count <-rbinom(length(true_count), size = true_count, prob = pdet_y) # pull count data with abundance * detection rate. 

# inference level 

  # construct summary
  tibble( 
    year = y, 
    doy = days, 
    mu_y = mu_y[y],
    sigma_y = sigma_y[y], 
    S_y = S_y[y], 
    N_y = N_y[y], 
    abundance = A_yd, 
    true_count = true_count, 
    obs_count = obs_count
  )
}
  )
return(sim.latent.det)
}
```

```{r}
sim2 <- sim.latent.w.det.fxn(pbloom = 0.3, pdet = 0.5, n_years = 10, days = 1:150, mu_peak = 90)

# try plot 
sim2 %>%
  filter(year %in% c(1,2,3,4,5)) %>%
  ggplot() +
  # geom_col(aes(x = doy, y = true_count, fill = factor(year)),
  #          position = "identity", width = bin_width * 0.9, alpha = 0.1) +
  geom_col(aes(x = doy, y = obs_count, fill = factor(year)), 
           position = "identity", alpha = 0.2) +
  geom_line(aes(x = doy, y = abundance, color = factor(year), linetype = factor(S_y)), linewidth = 1) +
  scale_linetype_manual(values = c("solid", "longdash"),
                        labels = c("Normal", "Superbloom"),
                        name = "Bloom type") +
  labs(title = "Flowering Abundance vs Observed Counts \n With Superbloom Latent Indicator Parameter & Uniform Imperfect Detection Added",
       x = "Day of Year", y = "Count / Expected Abundance", color = "Year", fill = "Year") +
  theme_minimal()
```
Check detection rates, besides the most extreme case where no obs exist, then we should recover 100% since its uniformly 1/2 the detection.
```{r}
check.superbloom.assessment.obs <- function(data = data){
  # first, summarize each year in our simulation
yearly_summary <- data %>%
  group_by(year) %>%
  summarise(
    total_obs_counts = sum(obs_count),
    peak_doy = doy[which.max(total_obs_counts)],
    obs_count_sd = sd(obs_count),
    S_y = unique(S_y),
    .groups = "drop"
  )

glm_superbloom <- glm(S_y ~ total_obs_counts,
                      #family = "binomial",
                      data = yearly_summary)

plot(glm_superbloom)

# predict superblooms via classification on our glm
yearly_summary <- yearly_summary %>%
  mutate(
    S_y_hat_prob = predict(glm_superbloom, type = "response"),
    S_y_hat = ifelse(S_y_hat_prob > 0.5, 1, 0)
  )

# confusion matrix 
confusion_matrix <- table(True = yearly_summary$S_y, Predicted = yearly_summary$S_y_hat)
print(confusion_matrix)
print(paste0((mean(yearly_summary$S_y == yearly_summary$S_y_hat)*100), "% Accuracy in Predicting SuperBlooms")) # overall accuracy
return(mean(yearly_summary$S_y == yearly_summary$S_y_hat)*100)
}

check.superbloom.assessment.obs(sim2)

# check for these fixed params with all detection rates:
det_rate_v <- seq(0, 1, by = 0.05)
results_df <- tibble()
for(i in 1:length(det_rate_v)){
  obs_simu <- sim.latent.w.det.fxn(pdet = det_rate_v[i], pbloom = 0.3, n_years = 1000, days = 1:150, mu_peak = 90)
  perc_acc <- check.superbloom.assessment.obs(obs_simu)
  results_df <- bind_rows(results_df, 
                        tibble(
                          pdet = det_rate_v[i],
                          accuracy = perc_acc
                        ))
}
```
### Alternative Method: Z-score Assessment
Since we're interested in just looking at the density of records (counts), we could try to use z-scores to extract candidate superblooms. This would normalize within year effects, possibly allowing us to extract the potential superblooms (Vaughn's idea). 

Z-scores here will highlight unusually high flowering events relative to the long-term mean at each doy. For a given doy, the z-score is:
$$
Z_{y,d} = \frac{obs \ count_{y,d} - \mu_d}{\sigma_d}
$$
where, $\mu_d$ is the mean obs count across all years at day $d$
        $\sigma_d$ is the standard deviation of obs count across all years at day $d$
        
        
```{r}
  sim_data <- sim.latent.w.det.fxn(n_years = 10,days = 1:150, mu_peak = 100, pbloom = 0.2, y_var = 10)
  # Compute mean and sd per doy 
  obs_stats <- sim_data %>% 
    group_by(doy) %>% 
    summarize(
      mean_obs = mean(obs_count), 
      sd_obs = sd(obs_count), 
      .groups = "drop"
    )
  
  # join and compute the z-scores
  sim_z_obs <- sim_data %>% 
    left_join(obs_stats, by = "doy") %>% 
    mutate(
      z_obs = (obs_count - mean_obs) / sd_obs
    )
  
  # summarize at year level, here we can use mean z-score or total z-score (per year), the total will better reflect shape and size
  z_summary <- sim_z_obs %>% 
    group_by(year) %>% 
    summarize(
      total_z = sum(z_obs, na.rm = TRUE),
      mean_z =  mean(z_obs, na.rm = TRUE), 
      S_y = unique(S_y), 
      .groups = "drop"
    )
  
  # detection of z-score outliers 
  z_cutoff <- mean(z_summary$total_z) + 1 * sd(z_summary$total_z)
  z_summary <- z_summary %>% 
    mutate(
      classified_superbloom = total_z > z_cutoff
    )
  
  
  
  ggplot(z_summary, aes(x = total_z, fill = as.factor(S_y))) +
    geom_density(alpha = 0.6) +
    geom_vline(xintercept = z_cutoff, linetype = "dashed", color = "red") +
    labs(title = "Distribution of Total Z by True Bloom Status",
         fill = "S_y")
  table(z_summary$S_y, z_summary$classified_superbloom)
```
```{r}
sim_data <- sim.latent.w.det.fxn(n_years = 7,days = 1:150, mu_peak = 100, pbloom = 0.42, y_var = 0)
# Set a z-score threshold and minimum number of outlier DOYs
z_thresh <- 2
min_outlier_doys <- 20

  obs_stats <- sim_data %>% 
    group_by(doy) %>% 
    summarize(
      mean_obs = mean(obs_count), 
      sd_obs = sd(obs_count), 
      .groups = "drop"
    )
  
  # join and compute the z-scores
  sim_z_obs <- sim_data %>% 
    left_join(obs_stats, by = "doy") %>% 
    mutate(
      z_obs = (obs_count - mean_obs) / sd_obs
    )

# Identify DOY-level outliers
sim_z_obs <- sim_z_obs %>%
  mutate(
    outlier_doy = z_obs > z_thresh
  )

# Count number of outlier DOYs per year
year_outlier_counts <- sim_z_obs %>%
  group_by(year) %>%
  summarize(
    n_outlier_doys = sum(outlier_doy, na.rm = TRUE),
    S_y = unique(S_y),  # keep bloom status
    .groups = "drop"
  ) %>%
  mutate(
    classified_superbloom = n_outlier_doys >= min_outlier_doys
  )

# Visualize
ggplot(year_outlier_counts, aes(x = n_outlier_doys, fill = as.factor(S_y))) +
  geom_histogram(binwidth = 1, alpha = 0.7, position = "identity") +
  geom_vline(xintercept = min_outlier_doys, linetype = "dashed", color = "red") +
  labs(title = "Number of Outlier DOYs per Year",
       x = "Number of DOYs with Z > 2",
       fill = "True Superbloom Status (S_y)")

# Check performance
table(year_outlier_counts$S_y, year_outlier_counts$classified_superbloom)
```


Additionally, we'll use an ROC curve to create the threshold of detection for superblooms. 
```{r}
sim_data <- sim.latent.w.det.fxn(n_years = 100)

# compute per-DOY stats from observed counts (across all years)
obs_stats <- sim_data %>% 
  group_by(doy) %>% 
  summarize(
    mean_obs = mean(obs_count), 
    sd_obs = sd(obs_count), 
    .groups = "drop"
  )

# join stats to original data and compute z-scores
sim_z_obs <- sim_data %>% 
  left_join(obs_stats, by = "doy") %>% 
  mutate(
    z_obs = (obs_count - mean_obs) / sd_obs
  )
# try using a ROC curve to generate the best case detection threshold for our z-score
sim_z_summary <- sim_z_obs %>% 
  group_by(year, S_y) %>% 
  summarize(mean_z_obs = mean(z_obs, na.rm = TRUE), .groups = "drop")

# try building an ROC curve 
roc_curve <- roc(sim_z_summary$S_y, sim_z_summary$mean_z_obs)
plot(roc_curve)
auc(roc_curve)
# choose optimal 
thresh <- as.numeric(coords(roc_curve, "best", ret = "threshold"))

# create a classifier at the year level 
classifier_obs <- sim_z_obs %>% 
  group_by(year, S_y) %>% 
  summarize(
    mean_z_obs = mean(z_obs, na.rm = TRUE),
    .groups = "drop"
  ) %>% 
  mutate(
    classified_superbloom = ifelse(mean_z_obs > thresh, 1, 0)
  )

# eval classifier 
confusion_matrix_obs <- table(True = classifier_obs$S_y, 
                              Predicted = classifier_obs$classified_superbloom)

confusion_matrix_obs
```
While this certaintly works well, its not useful when considering that we're trying to predict superblooms from untrained data. Technically we could use this if the simulation was realistic...that would require some parameterization. Lets try additional methods instead.

Use Mixture Modeling on Z-scores
Assume two latent distributions (superbloom vs normal) and fit a mixture model to the annual mean_z_obs values
```{r}
library(mclust)
# run simu
sim_data <- sim.latent.w.det.fxn(n_years = 10)

obs_stats <- sim_data %>%
  group_by(doy) %>%
  summarize(
    mean_obs = mean(obs_count), 
    sd_obs = sd(obs_count), 
    .groups = "drop"
  )

sim_z_obs <- sim_data %>%
  left_join(obs_stats, by = "doy") %>%
  mutate(
    z_obs = (obs_count - mean_obs) / sd_obs
  )

# First: Compute year-level z-scores
sim_z_summary <- sim_z_obs %>%
  group_by(year, S_y) %>%
  summarize(mean_z_obs = mean(z_obs, na.rm = TRUE), .groups = "drop")

# Extract numeric vector for clustering
mean_z_obs <- sim_z_summary$mean_z_obs


# Fit Gaussian mixture model
library(mclust)
model <- Mclust(mean_z_obs, G = 2)

# Store predictions
sim_z_summary$predicted_class <- model$classification

# change factors to match previous notation (0 = regular bloom, 1 = superbloom)
sim_z_summary <- sim_z_summary %>% 
  mutate(predicted_bloom_class = ifelse(predicted_class == 1, 0, 1))

table(sim_z_summary$S_y, sim_z_summary$predicted_bloom_class,
      dnn = c("True Superbloom (S_y)", "Predicted Class"))

# try plot 
sim_data %>%
  filter(year %in% c(7,8)) %>%
  ggplot() +
  # geom_col(aes(x = doy, y = true_count, fill = factor(year)),
  #          position = "identity", width = bin_width * 0.9, alpha = 0.1) +
  geom_col(aes(x = doy, y = obs_count, fill = factor(year)), 
           position = "identity", alpha = 0.2) +
  geom_line(aes(x = doy, y = abundance, color = factor(year), linetype = factor(S_y)), linewidth = 1) +
  scale_linetype_manual(values = c("solid", "longdash"),
                        labels = c("Normal", "Superbloom"),
                        name = "Bloom type") +
  labs(title = "Flowering Abundance vs Observed Counts \n With Superbloom Latent Indicator Parameter & Uniform Imperfect Detection Added",
       x = "Day of Year", y = "Count / Expected Abundance", color = "Year", fill = "Year") +
  theme_minimal()
```
Try with a glm 
```{r}
sim_data <- sim.latent.w.det.fxn(n_years = 100, pbloom = 0.2)

sim_data_summary <- sim_data %>% 
  group_by(year) %>% 
  mutate(S_y = unique(S_y)) %>% 
  select(year, S_y)
# fit the glm 
glm_model <- glm(obs_count ~ poly(doy, degree = 2), 
                 family = "poisson", 
                 data = sim_data)

summary(glm_model)

sim_data <- sim_data %>% 
  mutate(residuals = glm_model$residuals)

year_residuals <- sim_data %>% 
  group_by(year) %>% 
  summarize(
    mean_residual = mean(residuals),
    .groups = "drop"
  )

mean_res <- mean(year_residuals$mean_residual)
sd_res <- sd(year_residuals$mean_residual)

# Set threshold
threshold <- mean_res + 2 * sd_res

year_residuals <- year_residuals %>% 
  mutate(superbloom_classified = ifelse(mean_residual > threshold, 1, 0))

# Merge with true bloom status
classification_results <- year_residuals %>% 
  select(year, superbloom_classified, S_y) %>% 
  left_join(sim_data_summary, by = "year") %>% 
  distinct()

# Create confusion matrix
confusion_matrix <- table(classification_results$superbloom_classified, 
                         classification_results$S_y)

print(confusion_matrix)
```





### Add in temporal trend in detection
Adjust pdet so that it increases in later years to mimic iNaturalist data 
```{r}
sim.latent.w.inc.det <- function(n_years = 30, # number of years to simu
                         days = 1:365, # doy 
                         mu_peak = 100, # center for mean flowering
                         dur = 12, # duration of flowering 
                         y_var = 0, # variability per year
                         alpha_0 = 9, # standard bloom
                         alpha_1 = 2.5, # additional bloom param
                         pbloom = 0.2, # the prob of superbloom
                         min_det = 0.2,
                         max_det = 0.8# the observation rate 
                         ){
# define temporal parms
n_years <- n_years
days <- days

# set up simu parms (shifting to hyperparameters)
mu_mu <- rnorm(1, mean = mu_peak, sd = 10) # mean DOY peak across years
sigma_mu <- abs(rnorm(1, mean = y_var, sd = 5)) # variability across years
mu_sigma <- rnorm(1, mean = dur, sd = 3) # spread/duration of flowering 
sigma_sigma <- abs(rnorm(1, mean = 0, sd = 3)) # spread/duration of flowering
alpha_0 <- alpha_0 # means that N_y ~ exp(9) in normal years
alpha_1 <- alpha_1 # means that N_y ~ exp(9 + 2.5) in superbloom years

calibrate_bloom_beta <- function(mean, fixed_shape1 = 2) {
  if (mean <= 0 || mean >= 1) stop("mean must be strictly between 0 and 1")
  shape1 <- fixed_shape1
  shape2 <- shape1 * (1 - mean) / mean
  return(list(pshape1 = shape1, pshape2 = shape2))
}
bloom_params <- calibrate_bloom_beta(mean = pbloom, fixed_shape1 = 2)
p_bloom <- rbeta(1, shape1 = bloom_params$pshape1, shape2 = bloom_params$pshape2) # beta distribution, the conjugate prior for the Bernoulli and Binomial, 
# year specific parameters
mu_y <- rnorm(n_years, mean = mu_mu, sd = sigma_mu) # draw each year's flowering mean mu_y for Normal Distributions defined by the hyperparameters mu_mu & sigma_mu
sigma_y <- abs(rnorm(n_years, mean = mu_sigma, sd = sigma_sigma)) # draw each year's standard deviation sigma_y from Normal distributions defined by the hyperparameters mu_sigma and sigma_sigma
S_y <- rbinom(n_years, size = 1, prob = p_bloom) # draw from a binomial distribution for whether this should be a superbloom year, given the probability of the superbloom. 

log_N_y <- alpha_0 + alpha_1 * S_y # Determine log-abundance of flowering
N_y <- exp(log_N_y) # Exponentiate

# simulate count data with latent superbloom indicator S_y
sim.latent.det <- purrr::map_dfr(1:n_years, function(y){
  doy_density <- dnorm(days, mean = mu_y[y], sd = sigma_y[y]) # pdf over doy 
  A_yd <- N_y[y] * doy_density # expected abd lambda_{y,d}
  true_count <- rpois(length(A_yd), lambda = A_yd) # Poisson count draw
  # Helper function to calibrate beta distribution for desired mean
  
calibrate_beta <- function(mean, fixed_shape1 = 10) { # build a internal fxn that allows for setting the detection rate as a percentage.
  shape1 <- fixed_shape1
  shape2 <- shape1 * (1 - mean) / mean
  return(list(shape1 = shape1, shape2 = shape2))
}
# beta_params <- calibrate_beta(pdet)  # Allows for input of decimal percentage
# pdet_y <- rbeta(n_years, shape1 = beta_params$shape1, shape2 = beta_params$shape2) # Vary across years, but have an expected value around pdet input
# create trend of sampling throughout the years
pdet_trend <- seq(min_det, max_det, length.out = n_years)
# calibrate detection per year
pdet_y <- map_dbl(pdet_trend, function(p) {
  beta_params <- calibrate_beta(p)
  rbeta(1, shape1 = beta_params$shape1, shape2 = beta_params$shape2)
})

obs_count <-rbinom(length(true_count), size = true_count, prob = pdet_y[y]) # pull count data with abundance * detection rate. 

# inference level 

  # construct summary
  tibble( 
    year = y, 
    doy = days, 
    mu_y = mu_y[y],
    sigma_y = sigma_y[y], 
    S_y = S_y[y], 
    N_y = N_y[y], 
    abundance = A_yd, 
    true_count = true_count, 
    obs_count = obs_count, 
    pdet_y[y]
  )
}
  )
return(sim.latent.det)
}
```
Try simu
```{r}
set.seed(123)
simu_det <- sim.latent.w.inc.det(min_det = 0.1, max_det = 0.4, pbloom = 0.05)
# compute per-DOY stats from observed counts (across all years)
obs_stats <- simu_det %>% 
  group_by(doy) %>% 
  summarize(
    mean_obs = mean(obs_count), 
    sd_obs = sd(obs_count), 
    .groups = "drop"
  )

# join stats to original data and compute z-scores
sim_z_obs <- simu_det %>% 
  left_join(obs_stats, by = "doy") %>% 
  mutate(
    z_obs = (obs_count - mean_obs) / sd_obs
  )
# try using a ROC curve to generate the best case detection threshold for our z-score
sim_z_summary <- sim_z_obs %>% 
  group_by(year, S_y) %>% 
  summarize(mean_z_obs = mean(z_obs, na.rm = TRUE), .groups = "drop")

# try building an ROC curve 
roc_curve <- roc(sim_z_summary$S_y, sim_z_summary$mean_z_obs)
plot(roc_curve)
auc(roc_curve)
# choose optimal 
thresh <- as.numeric(coords(roc_curve, "best", ret = "threshold"))

# create a classifier at the year level 
classifier_obs <- sim_z_obs %>% 
  group_by(year, S_y) %>% 
  summarize(
    mean_z_obs = mean(z_obs, na.rm = TRUE),
    .groups = "drop"
  ) %>% 
  mutate(
    classified_superbloom = ifelse(mean_z_obs > thresh, 1, 0)
  )

# eval classifier 
confusion_matrix_obs <- table(True = classifier_obs$S_y, 
                              Predicted = classifier_obs$classified_superbloom)

confusion_matrix_obs
```


Try a linear model to find the obs trend overtime and use this to weight 
```{r}
# fit a model of expected total obs over time
effort_model <- lm(total_obs ~ year, data = effort_by_year)

# Predict effort baseline
effort_by_year$expected_obs <- predict(effort_model)

# compute effort index 
effort_by_year$effort_index <- effort_by_year$total_obs / effort_by_year$expected_obs

# Join effort back in and normalize obs_count 
sim_data_effort <- sim_data %>% 
  left_join(effort_by_year, by = "year") %>% 
  mutate(obs_effort_norm = obs_count / effort_index)

# try running
# compute cross-year z-scores per doy 
sim_data_z <- sim_data_effort %>% 
  group_by(doy) %>% 
  mutate(
    mean_doy = mean(obs_effort_norm, na.rm = TRUE), 
    sd_doy = sd(obs_effort_norm, na.rm = TRUE), 
    z_score = ifelse(sd_doy > 0, 
                     (obs_effort_norm - mean_doy) / sd_doy, 
                     0)
  )

# summarize by year
yearly_z_summary <- sim_data_z %>% 
  group_by(year, S_y) %>% 
  summarize(
    sum_z = sum(z_score, na.rm = TRUE), 
    max_z = max(z_score, na.rm = TRUE), 
    .groups = "drop"
  )

yearly_z_summary %>%
  mutate(predicted_S_y = as.integer(sum_z > 3)) %>%
  count(True = S_y, Predicted = predicted_S_y)
```

### Create a Spatial Matrix to mimic sampling across space
Here we'll assume that the data generated is what consitutes to the general geographic space, and that sampling across this space differs
```{r}
sim.latent.w.det.spatial <- function(n_years = 30,
                                     days = 1:150,
                                     mu_peak = 100,
                                     dur = 12,
                                     y_var = 0,
                                     alpha_0 = 9,
                                     alpha_1 = 2.5,
                                     pbloom = 0.2,
                                     pdet = 0.5,
                                     grid_n = 5) {
  # Temporal hyperparameters
  mu_mu <- rnorm(1, mean = mu_peak, sd = 10)
  sigma_mu <- abs(rnorm(1, mean = y_var, sd = 5))
  mu_sigma <- rnorm(1, mean = dur, sd = 3)
  sigma_sigma <- abs(rnorm(1, mean = 0, sd = 3))

  # Superbloom probability
  calibrate_bloom_beta <- function(mean, fixed_shape1 = 2) {
    shape1 <- fixed_shape1
    shape2 <- shape1 * (1 - mean) / mean
    list(pshape1 = shape1, pshape2 = shape2)
  }
  bloom_params <- calibrate_bloom_beta(mean = pbloom)
  p_bloom <- rbeta(1, shape1 = bloom_params$pshape1, shape2 = bloom_params$pshape2)

  # Year-specific flowering parameters
  mu_y <- rnorm(n_years, mean = mu_mu, sd = sigma_mu)
  sigma_y <- abs(rnorm(n_years, mean = mu_sigma, sd = sigma_sigma))
  S_y <- rbinom(n_years, 1, prob = p_bloom)
  log_N_y <- alpha_0 + alpha_1 * S_y
  N_y <- exp(log_N_y)

  # Precompute shared true counts (same across space)
  true_data <- purrr::map_dfr(1:n_years, function(y) {
    doy_density <- dnorm(days, mean = mu_y[y], sd = sigma_y[y])
    A_yd <- N_y[y] * doy_density
    true_count <- rpois(length(A_yd), lambda = A_yd)
    tibble(year = y, doy = days, true_count = true_count, abundance = A_yd,
           mu_y = mu_y[y], sigma_y = sigma_y[y], S_y = S_y[y], N_y = N_y[y])
  })

  # Generate spatially varying detection rates
  calibrate_beta <- function(mean, fixed_shape1 = 10) {
    shape1 <- fixed_shape1
    shape2 <- shape1 * (1 - mean) / mean
    list(shape1 = shape1, shape2 = shape2)
  }
  beta_params <- calibrate_beta(pdet)

  spatial_grid <- expand.grid(x = 1:grid_n, y = 1:grid_n) %>%
    mutate(cell_id = paste0("cell_", x, "_", y),
           pdet = rbeta(n = n(), shape1 = beta_params$shape1, shape2 = beta_params$shape2))

  # Simulate observed counts for each spatial cell
  sim_obs <- purrr::map_dfr(1:nrow(spatial_grid), function(i) {
    cell <- spatial_grid[i, ]
    pdet_c <- cell$pdet

    true_data %>%
      mutate(obs_count = rbinom(n(), size = true_count, prob = pdet_c),
             cell_id = cell$cell_id,
             x = cell$x,
             y = cell$y,
             pdet = pdet_c)
  })

  return(sim_obs)
}
```

Run simulation
```{r}
spatial_sim <- sim.latent.w.det.spatial(pbloom = 0.2, pdet = 0.5, grid_n = 5) # ngrid is 5x5 = 25 spatial cells

# create a visual for comparison 
spatial_sim %>%
  filter(year %in% c(1)) %>%
  ggplot() +
  # geom_col(aes(x = doy, y = true_count, fill = factor(year)),
  #          position = "identity", width = bin_width * 0.9, alpha = 0.1) +
  geom_col(aes(x = doy, y = obs_count, fill = factor(cell_id)), 
           position = "identity", alpha = 0.1) +
  geom_line(aes(x = doy, y = abundance, color = factor(year), linetype = factor(S_y)), linewidth = 1) +
  scale_linetype_manual(values = c("solid", "longdash"),
                        labels = c("Normal", "Superbloom"),
                        name = "Bloom type") +
  labs(title = "Flowering Abundance vs Observed Counts \n With Superbloom Latent Indicator Parameter & Uniform Imperfect Detection Added \n Across Spatial Cells",
       x = "Day of Year", y = "Count / Expected Abundance", color = "Year", fill = "Year") +
  theme_minimal()
```




### Testing more stuff
```{r}
sim_data <- sim.latent.w.det.fxn(n_years = 100)
sim_data_weekly <- sim_data %>%
  mutate(week = ceiling(doy / 7)) %>%
  group_by(year, week) %>%
  summarize(weekly_obs = sum(obs_count), .groups = "drop")

features <- sim_data_weekly %>%
  group_by(year) %>%
  summarize(
    total_obs = sum(weekly_obs),
    peak_week = week[which.max(weekly_obs)],
    peak_val = max(weekly_obs),
    spread = sd(weekly_obs),
    entropy = -sum((weekly_obs / sum(weekly_obs)) * log((weekly_obs + 1e-10) / sum(weekly_obs))),
    .groups = "drop"
)

true_labels <- sim_data %>%
  select(year, S_y) %>%
  distinct()

features <- left_join(features, true_labels, by = "year")

model <- glm(S_y ~ total_obs, 
             data = features, 
             family = "binomial")

features$predicted_prob <- predict(model, type = "response")
features$predicted_class <- as.integer(features$predicted_prob > 0.5)

# Confusion matrix
table(Predicted = features$predicted_class, Actual = features$S_y)
library(pROC)
roc_obj <- roc(features$S_y, features$predicted_prob)
auc(roc_obj)

gmm <- Mclust(features$total_obs, G = 2)
features$gmm_class <- gmm$classification

```
```{r}
sim_data <- sim.latent.w.inc.det(n_years = 100, y_var = 0)
# first, summarize each year in our simulation
yearly_summary <- sim_data %>%
  group_by(year) %>%
  summarise(
    obs_counts = sum(obs_count),
    peak_doy = doy[which.max(obs_count)],
    count_sd = sd(obs_count),
    S_y = unique(S_y),
    .groups = "drop"
  )

glm_superbloom <- glm(S_y ~ obs_counts + peak_doy + count_sd,
                      family = "binomial",
                      data = yearly_summary)

summary(glm_superbloom)

# predict superblooms via classification on our glm
yearly_summary <- yearly_summary %>%
  mutate(
    S_y_hat_prob = predict(glm_superbloom, type = "response"),
    S_y_hat = ifelse(S_y_hat_prob > 0.5, 1, 0)
  )

# confusion matrix 
confusion_matrix <- table(True = yearly_summary$S_y, Predicted = yearly_summary$S_y_hat)
print(confusion_matrix)
print(paste0((mean(yearly_summary$S_y == yearly_summary$S_y_hat)*100), "% Accuracy in Predicting SuperBlooms")) # overall accuracy
```

```{r}
# Define the function
sim.latent.w.det.spatial2 <- function(
  n_years = 30,
  days = 1:150,
  mu_peak = 100,
  dur = 12,
  y_var = 0,
  alpha_0 = 9,
  alpha_1 = 2.5,
  pbloom = 0.2,
  pdet = 0.5,
  grid_n = 5,
  dirichlet_conc = 1e6 # smaller = more uneven site abundances
) {
  # Temporal hyperparameters
  mu_mu <- rnorm(1, mean = mu_peak, sd = 10)
  sigma_mu <- abs(rnorm(1, mean = y_var, sd = 5))
  mu_sigma <- rnorm(1, mean = dur, sd = 3)
  sigma_sigma <- abs(rnorm(1, mean = 0, sd = 3))

  # Superbloom probability
  calibrate_bloom_beta <- function(mean, fixed_shape1 = 2) {
    shape1 <- fixed_shape1
    shape2 <- shape1 * (1 - mean) / mean
    list(pshape1 = shape1, pshape2 = shape2)
  }
  bloom_params <- calibrate_bloom_beta(mean = pbloom)
  p_bloom <- rbeta(1, shape1 = bloom_params$pshape1, shape2 = bloom_params$pshape2)

  # Year-specific flowering parameters
  mu_y <- rnorm(n_years, mean = mu_mu, sd = sigma_mu)
  sigma_y <- abs(rnorm(n_years, mean = mu_sigma, sd = sigma_sigma))
  S_y <- rbinom(n_years, 1, prob = p_bloom)
  log_N_y <- alpha_0 + alpha_1 * S_y
  N_y <- exp(log_N_y)

  # Spatial grid
  calibrate_beta <- function(mean, fixed_shape1 = 10) {
    shape1 <- fixed_shape1
    shape2 <- shape1 * (1 - mean) / mean
    list(shape1 = shape1, shape2 = shape2)
  }
  beta_params <- calibrate_beta(pdet)
  spatial_grid <- expand.grid(x = 1:grid_n, y = 1:grid_n) %>%
    mutate(
      cell_id = paste0("cell_", x, "_", y),
      pdet = rbeta(n = n(), shape1 = beta_params$shape1, shape2 = beta_params$shape2)
    )

  n_cells <- nrow(spatial_grid)

  # Generate data for each year
  sim_obs <- purrr::map_dfr(1:n_years, function(y) {
    # Daily expected abundance (density)
    doy_density <- dnorm(days, mean = mu_y[y], sd = sigma_y[y])

    # Dirichlet proportions across cells
    prop_cells <- MCMCpack::rdirichlet(1, rep(dirichlet_conc, n_cells))[1,]

    # For each cell
    daily_results <- purrr::map_dfr(1:n_cells, function(i) {
      cell <- spatial_grid[i, ]
      pdet_c <- cell$pdet
      prop_c <- prop_cells[i]

      # Expected abundance curve for this cell
      A_d_c <- N_y[y] * prop_c * doy_density

      # Simulate counts
      true_count <- rpois(length(A_d_c), lambda = A_d_c)
      obs_count <- rbinom(length(A_d_c), size = true_count, prob = pdet_c)

      tibble(
        year = y,
        doy = days,
        true_count = true_count,
        obs_count = obs_count,
        abundance = A_d_c,
        mu_y = mu_y[y],
        sigma_y = sigma_y[y],
        S_y = S_y[y],
        N_y = N_y[y],
        cell_id = cell$cell_id,
        x = cell$x,
        y = cell$y,
        pdet = pdet_c,
        prop_abundance = prop_c
      )
    })

    # Compute total true count per year & doy
    totals <- daily_results %>%
      group_by(year, doy) %>%
      summarize(
        true_count_total = sum(true_count),
        abundance_total = sum(abundance),
        .groups = "drop"
      )

    # Join back
    daily_results %>%
      left_join(totals, by = c("year", "doy"))
  })

  return(sim_obs)
}
```

```{r}
library(MCMCpack)
spatial_sim <- sim.latent.w.det.spatial2(pbloom = 0.2, pdet = 0.5, grid_n = 5) # ngrid is 5x5 = 25 spatial cells

# create a visual for comparison 
spatial_sim %>%
  filter(year %in% c(1)) %>%
  ggplot() +
  # geom_col(aes(x = doy, y = true_count, fill = factor(year)),
  #          position = "identity", width = bin_width * 0.9, alpha = 0.1) +
  geom_col(aes(x = doy, y = obs_count, fill = factor(cell_id)), 
           position = "stack", alpha = 0.9) +
  geom_line(aes(x = doy, y = abundance_total, color = factor(year), linetype = factor(S_y)), linewidth = 1) +
  scale_linetype_manual(values = c("solid", "longdash"),
                        labels = c("Normal", "Superbloom"),
                        name = "Bloom type") +
  labs(title = "Flowering Abundance vs Observed Counts \n With Superbloom Latent Indicator Parameter & Uniform Imperfect Detection Added \n Across Spatial Cells",
       x = "Day of Year", y = "Count / Expected Abundance", color = "Year", fill = "Year") +
  theme_minimal()
```

